ğŸ“Œ Architecture Overview



The system ingests data from multiple sources, normalizes it into a unified schema, exposes query APIs, and runs fully containerized with automated testing and cloud deployment.



High-Level Flow

CSV / APIs (CoinPaprika, CoinGecko)

&nbsp;       â†“

&nbsp;  Raw Tables (Postgres)

&nbsp;       â†“

&nbsp;Normalized Unified Schema

&nbsp;       â†“

&nbsp;  FastAPI Backend

&nbsp;       â†“

&nbsp;Public API Endpoints



ğŸ—‚ï¸ Project Structure

.

â”œâ”€â”€ api/                    # FastAPI application

â”‚   â”œâ”€â”€ main.py             # API entry point

â”‚   â”œâ”€â”€ db.py               # DB session \& engine

â”‚   â”œâ”€â”€ models.py           # SQLAlchemy models

â”‚   â””â”€â”€ services.py         # Data access logic

â”‚

â”œâ”€â”€ ingestion/              # ETL pipelines

â”‚   â”œâ”€â”€ runner.py           # ETL orchestrator

â”‚   â”œâ”€â”€ csv\_ingestion.py    # CSV ingestion

â”‚   â”œâ”€â”€ coinpaprika\_ingestion.py

â”‚   â””â”€â”€ coingecko\_ingestion.py

â”‚

â”œâ”€â”€ data/

â”‚   â””â”€â”€ sample.csv          # CSV source

â”‚

â”œâ”€â”€ tests/                  # Test suite

â”‚   â”œâ”€â”€ test\_api.py

â”‚   â”œâ”€â”€ test\_etl.py

â”‚   â””â”€â”€ test\_failure.py

â”‚

â”œâ”€â”€ docker-compose.yml

â”œâ”€â”€ Dockerfile

â”œâ”€â”€ start.sh                # Startup script (ETL + API)

â”œâ”€â”€ requirements.txt

â”œâ”€â”€ pytest.ini

â”œâ”€â”€ .github/workflows/ci.yml

â””â”€â”€ README.md



âš™ï¸ Tech Stack



Python 3.11



FastAPI



PostgreSQL



SQLAlchemy



Docker \& Docker Compose



GitHub Actions (CI)



AWS EC2 (Deployment)



ğŸ”„ Data Ingestion (ETL)

Sources



CSV (data/sample.csv)



CoinPaprika API



CoinGecko API



Features



Raw data stored in raw\_\* tables



Normalized unified schema



Type validation



Incremental ingestion (no reprocessing)



Idempotent writes



Resume-on-failure logic



Run metadata stored in etl\_runs



Execution



ETL runs:



Automatically on container startup



On-demand via:



docker-compose run --rm api python -m ingestion.runner





Hourly via cron on EC2



ğŸŒ API Endpoints

GET /health



Health check endpoint.



Returns:



Database connectivity



Last ETL run status



{

&nbsp; "status": "ok",

&nbsp; "db": "connected"

}



GET /data



Fetch normalized data.



Features



Pagination



Filtering



Metadata included



Example:



{

&nbsp; "request\_id": "...",

&nbsp; "api\_latency\_ms": 12,

&nbsp; "pagination": {

&nbsp;   "limit": 10,

&nbsp;   "offset": 0,

&nbsp;   "total": 12240

&nbsp; },

&nbsp; "data": \[...]

}



GET /stats



ETL run statistics.



Returns:



Records processed



Duration



Last success \& failure timestamps



Run metadata



GET /metrics



Prometheus-style metrics.



Example:



etl\_last\_run\_success 1

etl\_records\_total 12240

api\_status 1



ğŸ³ Dockerized System



The entire system runs via Docker.



Build \& Run

docker-compose up --build



Stop

docker-compose down



ğŸ§ª Testing



Tests run inside Docker, matching production.



Run Tests

docker-compose run --rm api pytest



Coverage



ETL transformations



Incremental ingestion



Failure recovery



API endpoints



ğŸ” CI/CD Pipeline (P2)



GitHub Actions automatically runs on:



Push



Pull Request



CI Steps



Checkout code



Build Docker images



Run test suite inside containers



CI config:



.github/workflows/ci.yml





âœ… All tests must pass for CI to succeed



â˜ï¸ Cloud Deployment (AWS EC2)



Deployed on AWS EC2 (Ubuntu)



Docker + Docker Compose installed



Public API exposed



Scheduled ETL via cron



Cron Job



Runs ETL hourly:



0 \* \* \* \* cd /home/ubuntu/kasparro-backend-amal-joseph \&\& docker-compose run --rm api python -m ingestion.runner >> etl.log 2>\&1



ğŸ” Secrets Management



API keys stored using environment variables



No secrets committed to source control



Docker .env supported

