ğŸ“Œ Architecture Overview



The system follows a standard ETL (Extract, Transform, Load) pattern to ensure data integrity and high availability.



High-Level Data Flow



Extraction: Ingests data from CSV files and external REST APIs.



Raw Storage: Data is first landed in raw\_\* tables in PostgreSQL.



Transformation: Normalizes disparate data formats into a Unified Schema.



Exhibition: A FastAPI application serves the normalized data via public endpoints.



graph TD

&nbsp;   A\[CSV Source] --> D\[PostgreSQL Raw Tables]

&nbsp;   B\[CoinPaprika API] --> D

&nbsp;   C\[CoinGecko API] --> D

&nbsp;   D --> E{Normalization Engine}

&nbsp;   E --> F\[Unified Schema]

&nbsp;   F --> G\[FastAPI Backend]

&nbsp;   G --> H\[Public API Endpoints]





âš™ï¸ Tech Stack



Language: Python 3.11



Framework: FastAPI



Database: PostgreSQL + SQLAlchemy (ORM)



Infrastructure: Docker \& Docker Compose



CI/CD: GitHub Actions



Cloud: AWS EC2 (Ubuntu)



ğŸ—‚ï¸ Project Structure



.

â”œâ”€â”€ api/                    # FastAPI application

â”‚   â”œâ”€â”€ main.py             # API entry point

â”‚   â”œâ”€â”€ db.py               # DB session \& engine

â”‚   â”œâ”€â”€ models.py           # SQLAlchemy models

â”‚   â””â”€â”€ services.py         # Data access logic

â”œâ”€â”€ ingestion/              # ETL pipelines

â”‚   â”œâ”€â”€ runner.py           # ETL orchestrator

â”‚   â”œâ”€â”€ csv\_ingestion.py    # CSV ingestion logic

â”‚   â”œâ”€â”€ coinpaprika\_ingestion.py

â”‚   â””â”€â”€ coingecko\_ingestion.py

â”œâ”€â”€ data/

â”‚   â””â”€â”€ sample.csv          # Local data source

â”œâ”€â”€ tests/                  # Pytest suite

â”‚   â”œâ”€â”€ test\_api.py

â”‚   â”œâ”€â”€ test\_etl.py

â”‚   â””â”€â”€ test\_failure.py

â”œâ”€â”€ docker-compose.yml      # Orchestration

â”œâ”€â”€ Dockerfile              # Backend container definition

â”œâ”€â”€ start.sh                # Startup script (ETL + API)

â”œâ”€â”€ requirements.txt        # Python dependencies

â””â”€â”€ .github/workflows/ci.yml # GitHub Actions config





ğŸ”„ Data Ingestion (ETL)



The ETL engine is built for reliability and scale.



Key Features



Incremental Ingestion: Skips already processed data to save resources.



Idempotency: Multiple runs produce the same state without duplicates.



Resume-on-failure: Logic to pick up where it left off after an interruption.



Metadata Tracking: Every run status is logged in the etl\_runs table.



Execution



The ETL runs automatically on container startup. To trigger it manually:



docker-compose run --rm api python -m ingestion.runner





ğŸŒ API Endpoints



Endpoint



Method



Description



/health



GET



System health, DB connection, and last ETL status.



/data



GET



Paginated and filtered access to normalized crypto data.



/stats



GET



Statistics on records processed and ETL run durations.



/metrics



GET



Prometheus-style metrics for monitoring.



ğŸ³ Getting Started (Docker)



Build and Run



docker-compose up --build





Running Tests



Tests run inside the container to ensure environment parity:



docker-compose run --rm api pytest





â˜ï¸ Deployment \& CI/CD



GitHub Actions (CI)



On every Push or Pull Request, the system:



Builds the Docker images.



Spins up a temporary Postgres instance.



Runs the full test suite (API, ETL, and Failure Recovery).



AWS EC2 Deployment



The system is deployed on an Ubuntu EC2 instance. The ETL is scheduled via cron to run hourly:



0 \* \* \* \* cd /home/ubuntu/project-root \&\& docker-compose run --rm api python -m ingestion.runner >> etl.log 2>\&1





ğŸ” Secrets Management



API keys and database credentials are managed via environment variables.



Use a .env file for local development (not committed to version control).



Production secrets are managed via GitHub Secrets or AWS Parameter Store.

